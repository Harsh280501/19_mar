{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2039a59-c439-437f-9986-badfc701c345",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa1630-33e5-4c14-803f-945c0a5f389a",
   "metadata": {},
   "source": [
    "\n",
    "Min-Max scaling is a common data preprocessing technique used to normalize the numeric features of a dataset. It rescales the values of a feature to a fixed range, usually between 0 and 1, based on the minimum and maximum values of that feature.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4556b8-4991-4c14-9c13-6aa747e4da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = [[500, 1],\n",
    "        [1000, 2],\n",
    "        [1500, 3],\n",
    "        [2000, 4]]\n",
    "\n",
    "\n",
    "min_max = MinMaxScaler()\n",
    "scaled_data = min_max.fit_transform(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880d4cb8-3c58-4e11-8d40-6fabaaf48c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.        ]\n",
      " [0.33333333 0.33333333]\n",
      " [0.66666667 0.66666667]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146fd32a-03e2-4a21-8e0a-846ef19358ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6f30a-52c9-4880-951b-c323e9863eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9021500-4f80-47f4-afe3-24b9637993e8",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf0db7-be98-46fc-8745-6e022551a335",
   "metadata": {},
   "source": [
    "\n",
    "The Unit Vector technique, also known as normalization or feature scaling by vector norm, is a method used to scale the feature vectors in a dataset to have a unit norm or length. It aims to normalize the feature vectors so that they all have the same scale but retain their direction or orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad9ffb5-b8a6-44bf-9fb1-76f2ca890b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data:\n",
      " [[0.5547002  0.83205029]\n",
      " [0.24253563 0.9701425 ]\n",
      " [0.51449576 0.85749293]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Example dataset\n",
    "data = [[2, 3],\n",
    "        [1, 4],\n",
    "        [3, 5]]\n",
    "\n",
    "# Applying Unit Vector scaling to the data\n",
    "scaled_data = normalize(data, norm='l2')\n",
    "\n",
    "# Printing the scaled values\n",
    "print(\"Scaled data:\\n\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfbfe42-305e-46b7-b0ca-e2dd2719c780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e14da5f8-c3d9-4d4a-8fcc-fd0ae8ce7b92",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7994cdf-943f-489a-b36a-6f786152e412",
   "metadata": {},
   "source": [
    "\n",
    "PCA, which stands for Principal Component Analysis, is a widely used dimensionality reduction technique in machine learning and data analysis. It helps to reduce the dimensionality of a dataset while retaining the most important information or patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58068953-dda6-49da-95b5-ea31f5bd9401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced data:\n",
      " [[-7.79422863  0.        ]\n",
      " [-2.59807621  0.        ]\n",
      " [ 2.59807621  0.        ]\n",
      " [ 7.79422863 -0.        ]]\n",
      "Explained variance ratio: [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9],\n",
    "                 [10, 11, 12]])\n",
    "\n",
    "# Creating an instance of PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fitting the PCA model to the data and transforming it\n",
    "reduced_data = pca.fit_transform(data)\n",
    "\n",
    "# Printing the reduced data and explained variance ratio\n",
    "print(\"Reduced data:\\n\", reduced_data)\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5541de1-feab-4b9a-b4bd-dddec954f701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fdbeccc-ed2f-4df1-9b08-77f908fe8a40",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064265c-debf-4388-9792-f568f9909c40",
   "metadata": {},
   "source": [
    "\n",
    "PCA can be used as a feature extraction technique in addition to being a dimensionality reduction method. In the context of feature extraction, PCA is used to transform the original features into a new set of features, called principal components, that capture the most important information or patterns in the data. These principal components can be considered as a compressed representation of the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e8c723-b3b7-44f3-b1ce-9029f5708191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed features:\n",
      " [[-2.26470281  0.4800266 ]\n",
      " [-2.08096115 -0.67413356]\n",
      " [-2.36422905 -0.34190802]\n",
      " [-2.29938422 -0.59739451]\n",
      " [-2.38984217  0.64683538]\n",
      " [-2.07563095  1.48917752]\n",
      " [-2.44402884  0.0476442 ]\n",
      " [-2.23284716  0.22314807]\n",
      " [-2.33464048 -1.11532768]\n",
      " [-2.18432817 -0.46901356]\n",
      " [-2.1663101   1.04369065]\n",
      " [-2.32613087  0.13307834]\n",
      " [-2.2184509  -0.72867617]\n",
      " [-2.6331007  -0.96150673]\n",
      " [-2.1987406   1.86005711]\n",
      " [-2.26221453  2.68628449]\n",
      " [-2.2075877   1.48360936]\n",
      " [-2.19034951  0.48883832]\n",
      " [-1.898572    1.40501879]\n",
      " [-2.34336905  1.12784938]\n",
      " [-1.914323    0.40885571]\n",
      " [-2.20701284  0.92412143]\n",
      " [-2.7743447   0.45834367]\n",
      " [-1.81866953  0.08555853]\n",
      " [-2.22716331  0.13725446]\n",
      " [-1.95184633 -0.62561859]\n",
      " [-2.05115137  0.24216355]\n",
      " [-2.16857717  0.52714953]\n",
      " [-2.13956345  0.31321781]\n",
      " [-2.26526149 -0.3377319 ]\n",
      " [-2.14012214 -0.50454069]\n",
      " [-1.83159477  0.42369507]\n",
      " [-2.61494794  1.79357586]\n",
      " [-2.44617739  2.15072788]\n",
      " [-2.10997488 -0.46020184]\n",
      " [-2.2078089  -0.2061074 ]\n",
      " [-2.04514621  0.66155811]\n",
      " [-2.52733191  0.59229277]\n",
      " [-2.42963258 -0.90418004]\n",
      " [-2.16971071  0.26887896]\n",
      " [-2.28647514  0.44171539]\n",
      " [-1.85812246 -2.33741516]\n",
      " [-2.5536384  -0.47910069]\n",
      " [-1.96444768  0.47232667]\n",
      " [-2.13705901  1.14222926]\n",
      " [-2.0697443  -0.71105273]\n",
      " [-2.38473317  1.1204297 ]\n",
      " [-2.39437631 -0.38624687]\n",
      " [-2.22944655  0.99795976]\n",
      " [-2.20383344  0.00921636]\n",
      " [ 1.10178118  0.86297242]\n",
      " [ 0.73133743  0.59461473]\n",
      " [ 1.24097932  0.61629765]\n",
      " [ 0.40748306 -1.75440399]\n",
      " [ 1.0754747  -0.20842105]\n",
      " [ 0.38868734 -0.59328364]\n",
      " [ 0.74652974  0.77301931]\n",
      " [-0.48732274 -1.85242909]\n",
      " [ 0.92790164  0.03222608]\n",
      " [ 0.01142619 -1.03401828]\n",
      " [-0.11019628 -2.65407282]\n",
      " [ 0.44069345 -0.06329519]\n",
      " [ 0.56210831 -1.76472438]\n",
      " [ 0.71956189 -0.18622461]\n",
      " [-0.0333547  -0.43900321]\n",
      " [ 0.87540719  0.50906396]\n",
      " [ 0.35025167 -0.19631173]\n",
      " [ 0.15881005 -0.79209574]\n",
      " [ 1.22509363 -1.6222438 ]\n",
      " [ 0.1649179  -1.30260923]\n",
      " [ 0.73768265  0.39657156]\n",
      " [ 0.47628719 -0.41732028]\n",
      " [ 1.2341781  -0.93332573]\n",
      " [ 0.6328582  -0.41638772]\n",
      " [ 0.70266118 -0.06341182]\n",
      " [ 0.87427365  0.25079339]\n",
      " [ 1.25650912 -0.07725602]\n",
      " [ 1.35840512  0.33131168]\n",
      " [ 0.66480037 -0.22592785]\n",
      " [-0.04025861 -1.05871855]\n",
      " [ 0.13079518 -1.56227183]\n",
      " [ 0.02345269 -1.57247559]\n",
      " [ 0.24153827 -0.77725638]\n",
      " [ 1.06109461 -0.63384324]\n",
      " [ 0.22397877 -0.28777351]\n",
      " [ 0.42913912  0.84558224]\n",
      " [ 1.04872805  0.5220518 ]\n",
      " [ 1.04453138 -1.38298872]\n",
      " [ 0.06958832 -0.21950333]\n",
      " [ 0.28347724 -1.32932464]\n",
      " [ 0.27907778 -1.12002852]\n",
      " [ 0.62456979  0.02492303]\n",
      " [ 0.33653037 -0.98840402]\n",
      " [-0.36218338 -2.01923787]\n",
      " [ 0.28858624 -0.85573032]\n",
      " [ 0.09136066 -0.18119213]\n",
      " [ 0.22771687 -0.38492008]\n",
      " [ 0.57638829 -0.1548736 ]\n",
      " [-0.44766702 -1.54379203]\n",
      " [ 0.25673059 -0.5988518 ]\n",
      " [ 1.84456887  0.87042131]\n",
      " [ 1.15788161 -0.69886986]\n",
      " [ 2.20526679  0.56201048]\n",
      " [ 1.44015066 -0.04698759]\n",
      " [ 1.86781222  0.29504482]\n",
      " [ 2.75187334  0.8004092 ]\n",
      " [ 0.36701769 -1.56150289]\n",
      " [ 2.30243944  0.42006558]\n",
      " [ 2.00668647 -0.71143865]\n",
      " [ 2.25977735  1.92101038]\n",
      " [ 1.36417549  0.69275645]\n",
      " [ 1.60267867 -0.42170045]\n",
      " [ 1.8839007   0.41924965]\n",
      " [ 1.2601151  -1.16226042]\n",
      " [ 1.4676452  -0.44227159]\n",
      " [ 1.59007732  0.67624481]\n",
      " [ 1.47143146  0.25562182]\n",
      " [ 2.42632899  2.55666125]\n",
      " [ 3.31069558  0.01778095]\n",
      " [ 1.26376667 -1.70674538]\n",
      " [ 2.0377163   0.91046741]\n",
      " [ 0.97798073 -0.57176432]\n",
      " [ 2.89765149  0.41364106]\n",
      " [ 1.33323218 -0.48181122]\n",
      " [ 1.7007339   1.01392187]\n",
      " [ 1.95432671  1.0077776 ]\n",
      " [ 1.17510363 -0.31639447]\n",
      " [ 1.02095055  0.06434603]\n",
      " [ 1.78834992 -0.18736121]\n",
      " [ 1.86364755  0.56229073]\n",
      " [ 2.43595373  0.25928443]\n",
      " [ 2.30492772  2.62632347]\n",
      " [ 1.86270322 -0.17854949]\n",
      " [ 1.11414774 -0.29292262]\n",
      " [ 1.2024733  -0.81131527]\n",
      " [ 2.79877045  0.85680333]\n",
      " [ 1.57625591  1.06858111]\n",
      " [ 1.3462921   0.42243061]\n",
      " [ 0.92482492  0.0172231 ]\n",
      " [ 1.85204505  0.67612817]\n",
      " [ 2.01481043  0.61388564]\n",
      " [ 1.90178409  0.68957549]\n",
      " [ 1.15788161 -0.69886986]\n",
      " [ 2.04055823  0.8675206 ]\n",
      " [ 1.9981471   1.04916875]\n",
      " [ 1.87050329  0.38696608]\n",
      " [ 1.56458048 -0.89668681]\n",
      " [ 1.5211705   0.26906914]\n",
      " [ 1.37278779  1.01125442]\n",
      " [ 0.96065603 -0.02433167]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "#X_standardized = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "X_standardized = scaler.fit(X)\n",
    "X_standardized = scaler.transform(X)\n",
    "\n",
    "# Apply PCA for feature extraction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Print the transformed features\n",
    "print(\"Transformed features:\\n\", X_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91faff40-9da0-44c6-a29b-f5e2b78a67c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "054b9119-a4ce-4d10-9ebb-89376ecf26aa",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00afc00e-969a-4961-a47f-4268bde681d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data:\n",
      " [[0.         1.         0.        ]\n",
      " [0.66666667 0.         0.66666667]\n",
      " [0.33333333 0.28571429 0.33333333]\n",
      " [1.         0.57142857 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example dataset\n",
    "data = [[10, 4.5, 30],\n",
    "        [20, 3.8, 40],\n",
    "        [15, 4.0, 35],\n",
    "        [25, 4.2, 45]]\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Print the scaled data\n",
    "print(\"Scaled data:\\n\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc436d-e008-4f8b-b0d5-b20a174720c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf7d8798-b657-4f0b-a6e7-f49b599c0461",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f63439c6-ee53-443e-8286-92543bd52951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [0.56703854 0.27809174 0.15486973]\n",
      "Reduced dataset:\n",
      " [[-0.67124338 -1.0825524   1.24392235]\n",
      " [ 2.76094766 -0.3810689  -0.40028267]\n",
      " [-0.30866636  1.99149472  0.29719392]\n",
      " [-1.78103793 -0.52787342 -1.14083359]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example dataset\n",
    "dataset = np.array([[100, 50, 200, 0.05, 0.01],\n",
    "                    [150, 40, 180, 0.04, 0.02],\n",
    "                    [120, 45, 220, 0.06, 0.03],\n",
    "                    [130, 55, 210, 0.07, 0.01]])\n",
    "\n",
    "# Preprocess the data by standardizing the features\n",
    "scaler = StandardScaler()\n",
    "scaled_dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# Create an instance of PCA with the desired number of components\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Fit PCA to the preprocessed data\n",
    "pca.fit(scaled_dataset)\n",
    "\n",
    "# Analyze explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "\n",
    "# Transform the data using PCA\n",
    "reduced_dataset = pca.transform(scaled_dataset)\n",
    "\n",
    "# Print the reduced dataset\n",
    "print(\"Reduced dataset:\\n\", reduced_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e21552-a142-4363-9e33-ec76cbec1e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67cf42e3-6d70-4630-862f-87f8fe905a29",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0dfcfa-9e41-439c-94fb-437f8fea95ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max scaled data:\n",
      " [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Given dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape the data to 2D as MinMaxScaler expects a 2D array\n",
    "data_reshaped = data.reshape(-1, 1)\n",
    "\n",
    "# Fit and transform the data using MinMaxScaler\n",
    "min_max_scaled_data = scaler.fit_transform(data_reshaped)\n",
    "\n",
    "# Flatten the scaled data to 1D array\n",
    "min_max_scaled_data = min_max_scaled_data.flatten()\n",
    "\n",
    "# Print the Min-Max scaled data\n",
    "print(\"Min-Max scaled data:\\n\", min_max_scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd09adb-e4b3-4d21-9514-0cdf3a5f2daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f27afff-8f32-42f7-944d-596010c7e851",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bbc6286-fd3e-43d7-8bde-e0f7198c43e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [5.61656653e-01 3.17805122e-01 1.20538225e-01 4.30805467e-33]\n",
      "Number of principal components to retain: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example dataset\n",
    "dataset = np.array([[170, 65, 30, 0, 120],\n",
    "                    [160, 55, 40, 1, 130],\n",
    "                    [175, 70, 35, 1, 125],\n",
    "                    [180, 75, 45, 0, 140]])\n",
    "\n",
    "# Preprocess the data by standardizing the features\n",
    "scaler = StandardScaler()\n",
    "scaled_dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# Create an instance of PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA to the preprocessed data\n",
    "pca.fit(scaled_dataset)\n",
    "\n",
    "# Analyze explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "\n",
    "# Determine the number of principal components to retain\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "num_components_to_retain = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "\n",
    "# Print the number of principal components to retain\n",
    "print(\"Number of principal components to retain:\", num_components_to_retain)\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e50f85-34c7-4b70-85e2-21f7b97acb2c",
   "metadata": {},
   "source": [
    "Will chose the number of PCA on basis of explained varriance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4df8e7-9248-4f55-909c-8f361b5e3cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
